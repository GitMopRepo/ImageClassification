{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch is an open source deep learning framework built to be flexible framework ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most pytorch codes are imperative that tell what and tell how. Imperative program performs computation at runtime\n",
    "- Imperative programs are more flexiblee because of python\n",
    "- Dynamic computation graph are built and rebuilt as necessary at runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Torch backend ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './MNIST/raw'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torchvision dataset images are in the range [0,1], and each channel of the tensor images needs to be normalized inthe range [-1,1] using mean and standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's DataLoader contain a few interesting options other than the dataset and batch size. For example we could use num_workers > 1 to use subprocesses to asynchronously load data or using pinned RAM\n",
    "#DataLoader combines a dataset and a sampler to provides an iterable over MNIST dataset.\n",
    "#train set is an instance of  MNIST in this calss that also lives inside torchvision package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For dataloader we are using dataloader constructor and passing train set along with batch size. Batch_size, which denotes the number of samples contained in each generated batch\n",
    "num_workers, which denotes the number of processes that generate batches in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "#To pass our data into our PyTorch models we need to convert it to a PyTorch Dataset. A Tensor Dataset in this case.\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root=root, train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root=root, train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model using PyTorch ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are inheriting from nn.Module. Combined with super().init() this creates a class that tracks the architecture and provides a lot of useful methods and attributes. \n",
    "It is mandatory to inherit from nn.Module when you are creating a class for your network\n",
    " The module automatically creates the weight and bias tensors which we will use in the forward method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #input Image -28 * 28 * 1               \n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        #self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2=nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.conv3=nn.Conv2d(in_channels=32,out_channels= 64, kernel_size=3)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc1=nn.Linear(in_features=1*1*64, out_features=120)\n",
    "        self.fc2=nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3=nn.Linear(in_features=84, out_features=10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=self.pool(F.relu(self.conv3(x))) \n",
    "\n",
    "        x = x.view(-1, 64 * 1 * 1)\n",
    "\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "       \n",
    "net=Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Loss function and Optimizer ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calculate the loss between the true label and the predicted label from the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Perform gradient calculations using backward() and then update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 - image count  1875 - Loss 0.049\n",
      "epoch 2 - image count  1875 - Loss 0.036\n",
      "Time for training using PyTorch 47.722747\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "for epoch in range(2):\n",
    "    running_loss=0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        images, labels=data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward \n",
    "        outputs= net(images)\n",
    "        #calculate the loss between the target and the actuals\n",
    "        loss= criterion(outputs, labels)\n",
    "        #Gradient calculation uisng backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "#calculate loss\n",
    "        running_loss+=loss.item()\n",
    "        if i%1875==1874:\n",
    "            print('epoch %d - image count %5d - Loss %.3f' % (epoch +1, i+1, running_loss/1875))      \n",
    "            running_loss = 0.0\n",
    "t1_end = perf_counter()\n",
    "print(\"Time for training using PyTorch %f\" %(t1_end-t1_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mnist_pyt.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy using PyTorch is 99.98 and execution time 2.48 seconds\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "total=0\n",
    "t1_start=perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    '''Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). \n",
    "    It will reduce memory consumption for computations that would otherwise have requires_grad=True.'''\n",
    "for  data in test_loader:\n",
    "        images. lables=data\n",
    "        outputs= net(images)\n",
    "        _,pred= torch.max(outputs,1)\n",
    "        total+=labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    \n",
    "t1_end=perf_counter()\n",
    "print(\"Eval accuracy using PyTorch is %.2f and execution time %.2f seconds\" %((100 * (correct / total)), (t1_end-t1_start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
